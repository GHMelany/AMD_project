{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GHMelany/AMD_project/blob/main/AMD%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037c7a35-dc65-4afb-90a6-f9f4034be2ec",
      "metadata": {
        "id": "037c7a35-dc65-4afb-90a6-f9f4034be2ec"
      },
      "source": [
        "PROJECT: Finding similar items. Implement a detector of pairs of similar book reviews\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6215a9-9085-4765-a717-f0a986b7d93a",
      "metadata": {
        "id": "7d6215a9-9085-4765-a717-f0a986b7d93a"
      },
      "source": [
        "LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3HwyqmtnhHY",
        "outputId": "c77c7729-c4bf-444a-c828-ba8eec0627be"
      },
      "id": "p3HwyqmtnhHY",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.12/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from langid) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall current version of PySpark\n",
        "!pip uninstall -y pyspark\n",
        "\n",
        "# install stable version of PySpark\n",
        "!pip install pyspark==3.4.1\n",
        "\n",
        "# set JAVA_HOME (Java 11)\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# start Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "print(\"Spark versione:\", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Ox_VtQRnoy",
        "outputId": "1c1ab040-45e5-445b-d077-e0c5f2db4009"
      },
      "id": "I9Ox_VtQRnoy",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyspark 3.4.1\n",
            "Uninstalling pyspark-3.4.1:\n",
            "  Successfully uninstalled pyspark-3.4.1\n",
            "Collecting pyspark==3.4.1\n",
            "  Using cached pyspark-3.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Installing collected packages: pyspark\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.4.1\n",
            "Spark versione: 3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cb996b71-c88f-4982-86f9-85f6d6786890",
      "metadata": {
        "id": "cb996b71-c88f-4982-86f9-85f6d6786890"
      },
      "outputs": [],
      "source": [
        "import re, html, langid\n",
        "import os\n",
        "import zipfile\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, HashingTF, MinHashLSH\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import explode\n",
        "import math\n",
        "from pyspark.ml.feature import HashingTF, MinHashLSH\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0deba4af-df2e-4efc-83cb-868cb23ec98c",
      "metadata": {
        "id": "0deba4af-df2e-4efc-83cb-868cb23ec98c",
        "outputId": "a7ab18d6-277c-4de9-f131-7be2c8f43e30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "amazon-books-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"melanygomez\"\n",
        "os.environ['KAGGLE_KEY'] = \"38db1cce93622035560027022e9cafc\"\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"amazon-books-reviews.zip\"\n",
        "extract_dir = \"amazon_books_reviews\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(extract_dir)\n",
        "\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n",
        "\n",
        "folder = \"amazon_books_reviews\"\n",
        "csv_path = os.path.join(folder, \"Books_rating.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHDQidBymq6K",
        "outputId": "d7ba42ee-6caa-4ac4-903f-22e3f8b0a71e"
      },
      "id": "PHDQidBymq6K",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon_books_reviews/books_data.csv\n",
            "amazon_books_reviews/Books_rating.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(csv_path, header=True, inferSchema=True, quote='\"', escape='\"')\n",
        "df = df.select(\"Id\", F.col(\"review/score\").alias(\"score\"), F.col(\"review/text\").alias(\"text\")).dropna(subset=[\"text\"])"
      ],
      "metadata": {
        "id": "atTIojo5m3XL"
      },
      "id": "atTIojo5m3XL",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(fraction=0.05, seed=42)\n",
        "print(\"Initial sample:\", df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA4fLLROV-d4",
        "outputId": "322107e8-bbf6-4d5a-bf22-d48e8aeb5239"
      },
      "id": "hA4fLLROV-d4",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial sample: 150324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HTML decoding + lowercase conversion\n",
        "decode_clean = udf(lambda txt: re.sub(r\"\\s+\", \" \", html.unescape(txt).lower()) if txt else \"\", StringType())\n",
        "df = df.withColumn(\"review_clean\", decode_clean(col(\"text\")))\n",
        "\n",
        "# remove special characters\n",
        "regex_clean = udf(lambda t: re.sub(r\"[^a-z0-9 ]+\", \" \", t) if t else \"\", StringType())\n",
        "df = df.withColumn(\"review_clean\", regex_clean(col(\"review_clean\")))\n"
      ],
      "metadata": {
        "id": "cli6rig0m6Fl"
      },
      "id": "cli6rig0m6Fl",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_udf = udf(lambda s: \"positive\" if s > 3 else \"negative\", StringType())\n",
        "df = df.withColumn(\"sentiment\", sentiment_udf(col(\"score\")))"
      ],
      "metadata": {
        "id": "4_B0MfWRm8oA"
      },
      "id": "4_B0MfWRm8oA",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in [\"tokens\", \"filtered_tokens\"]:\n",
        "    if c in df.columns:\n",
        "        df = df.drop(c)\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review_clean\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
        "stop_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "pipeline = Pipeline(stages=[tokenizer, stop_remover])\n",
        "df = pipeline.fit(df).transform(df)\n"
      ],
      "metadata": {
        "id": "6pfsttxvnAU4"
      },
      "id": "6pfsttxvnAU4",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"token_count\", F.size(col(\"filtered_tokens\")))\n",
        "df = df.filter((col(\"token_count\") >= 5) & (col(\"token_count\") <= 200))\n",
        "print(\"reviews after length filter:\", df.count())"
      ],
      "metadata": {
        "id": "UeIWFZ7OnAX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680e4672-3888-484e-9071-596e8913146f"
      },
      "id": "UeIWFZ7OnAX2",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reviews after length filter: 139612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_shingles(words, k=3):\n",
        "    if not words or len(words) < k:\n",
        "        return []\n",
        "    return list({\" \".join(words[i:i+k]) for i in range(len(words)-k+1)})\n",
        "\n",
        "shingles_udf = udf(lambda w: make_shingles(w, 3), ArrayType(StringType()))\n",
        "df = df.withColumn(\"shingles\", shingles_udf(col(\"filtered_tokens\"))).filter(F.size(col(\"shingles\")) > 0)"
      ],
      "metadata": {
        "id": "tCQ5nvIjnAbY"
      },
      "id": "tCQ5nvIjnAbY",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar(input_df, threshold=0.5, num_hash=20):\n",
        "    unique_terms = input_df.select(explode(\"shingles\")).distinct().count()\n",
        "    features_dim = 2 ** math.ceil(math.log2(unique_terms))\n",
        "\n",
        "    tf = HashingTF(inputCol=\"shingles\", outputCol=\"features\", numFeatures=features_dim, binary=True)\n",
        "    vectorized = tf.transform(input_df)\n",
        "\n",
        "    lsh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=num_hash)\n",
        "    model = lsh.fit(vectorized)\n",
        "\n",
        "    candidates = model.approxSimilarityJoin(vectorized, vectorized, threshold=threshold, distCol=\"jaccard_dist\")\n",
        "    pairs = (candidates\n",
        "             .filter(col(\"datasetA.Id\") > col(\"datasetB.Id\"))\n",
        "             .withColumn(\"similarity\", 1 - col(\"jaccard_dist\"))\n",
        "             .select(\"similarity\",\n",
        "                     col(\"datasetA.Id\").alias(\"docA\"),\n",
        "                     col(\"datasetB.Id\").alias(\"docB\"),\n",
        "                     col(\"datasetA.review_clean\").alias(\"reviewA\"),\n",
        "                     col(\"datasetB.review_clean\").alias(\"reviewB\"))\n",
        "             .dropDuplicates([\"docA\", \"docB\"]))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "P_UrAQsjnJYF"
      },
      "id": "P_UrAQsjnJYF",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtered by sentiment\n",
        "pos_reviews = df.filter(col(\"sentiment\") == \"positive\")\n",
        "neg_reviews = df.filter(col(\"sentiment\") == \"negative\")\n",
        "\n",
        "# similar pairs MinHash LSH\n",
        "pairs_pos = find_similar(pos_reviews, threshold=0.5)\n",
        "pairs_neg = find_similar(neg_reviews, threshold=0.5)\n",
        "\n",
        "# filter (0.8 - 0.95)\n",
        "mod_pos = pairs_pos.filter((col(\"similarity\") >= 0.5) & (col(\"similarity\") <= 0.95))\n",
        "mod_neg = pairs_neg.filter((col(\"similarity\") >= 0.5) & (col(\"similarity\") <= 0.95))\n",
        "\n",
        "# Print summary counts\n",
        "print(f\" Total similar positive pairs: {pairs_pos.count()}\")\n",
        "print(f\" Total similar negative pairs: {pairs_neg.count()}\")\n",
        "print(f\" similar positive pairs (0.5 - 0.95): {mod_pos.count()}\")\n",
        "print(f\" similar negative pairs (0.5 - 0.95): {mod_neg.count()}\")\n",
        "\n",
        "#Show examples of top similar pairs\n",
        "print(\"\\n 5 moderately similar positive pairs:\")\n",
        "mod_pos.orderBy(col(\"similarity\").desc()).show(5, truncate=False)\n",
        "\n",
        "print(\"\\n 5 moderately similar negative pairs:\")\n",
        "mod_neg.orderBy(col(\"similarity\").desc()).show(5, truncate=False)\n",
        "mod_neg.orderBy(col(\"similarity\").desc()).show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCEpdDg63w5M",
        "outputId": "984e286d-38f1-4d9f-d088-a10df1e807c0"
      },
      "id": "uCEpdDg63w5M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total similar positive pairs: 3761\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}